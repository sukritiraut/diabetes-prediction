---
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##  Building a model to predict whether a hospital patient (female) has diabetes based on patient information.

The diabetes.csv dataset consists of the following 9 variables:

1.  Pregnancies: the number of pregnancies the patient has experienced in her lifetime 

2.  Glucose: plasma glucose concentration (2 hour oral test)

3.  BloodPressure:  diastolic blood pressure (mm Hg)

4.  SkinThickness:  triceps skin fold thickness (mm)

5.  Insulin: 2 hour serum insulin (mu U/ml)

6.  BMI: Body mass index  (weight in kg/(height in m)^2)

7.  DiabetesPedigree: Diabetes pedigree function

8.  Age:  Age in years 

9.  Outcome:  0 if the patient does not have diabetes, 1 if the patient has diabetes 


## Read Data


```{r}
# Load relevant libraries (add here if needed)
library(car)
library(caret)
library(CombMSC)
library(bestglm)
library(glmnet)
library(boot)
library(rpart)
library(rpart.plot)
library(leaps)
library(MASS)

# Ensure that the sampling type is correct
RNGkind(sample.kind="Rejection")

# Set seed 
set.seed(0)

# Read the data
dataFull = read.csv("diabetes.csv", header=TRUE)

# Split data for training and testing
testRows = sample(nrow(dataFull), 0.2*nrow(dataFull))
dataTest = dataFull[testRows, ]
dataTrain = dataFull[-testRows, ]


```


**Treating all variables as quantitative variables.** 

## Full Model 

```{r}
# Code to fit logistic regression model (logit link function) model and display summary
model1 <- glm(Outcome~., family=binomial, data=dataTrain)
summary(model1) #summary table 

```

Coefficients that are statistically significant at the 95% confidence level  

```{r}
p_values = summary(model1)$coef[,4]
alpha = 0.05
p_values <= alpha
```

**List of significant coefficients: Intercept, Glucose, BMI, DiabetesPedigree, and Age**

**Explanation: They have very low p-values that are less than the alpha value of 0.05 (95% confidence interval.**


```{r}
odd <-exp(0.0335841)
odd
```

**The estimated coefficient of age which is 0.0335841 means that for one unit increase in the age of the patient,the log odds of pts having diabetes  increases by 0.0335841, holding all other variables constant. Holding other predictors constant, an increase of one year of Age will increase the odds of having diabetes by 3.415% (e0.0335841 âˆ’ 1 = 0.034151207). or one year increase in the age of the patient, the odds of the patient having diabetes increases by a factor of e^(0.0335841) = 1.03154, holding all other variables constant.**


```{r}
odd2 <- exp(0.0335841*5)-1
odd2
```

**Similarly, holding all other variables constant, an increase in age of 5 years will change the odds of the patient having diabetes by 18%**


 Deviance test for the overall regression of *model1*. What do you conclude using a significance level of 0.05? Include your reasoning.

```{r}
gstat = model1$null.deviance- deviance(model1)
p_value = 1-pchisq(gstat,length(coef(model1)) -1)
cbind(gstat, p_value )

```

**test statistic=314.064 and p-value=0**

**The null hypothesis is that the all model coeff except for intercept are 0 and the alternative hypothesis suggests that at least one of the coeff is non-zero. Based on the low p-value of 0 which is less than 0.05, we can reject the null hypothesis and conclude that at least one of coeffs is non-zero and thus the overall model has explanatory power. **


## Outlier and Overdispersion Detection 


```{r}
# Code to calculate and plot Cook's distance
# Calculating Cook's distances
cook=cooks.distance(model1)
# Plotting Cook's distances
plot(cook)
abline(h=1, col="red")



#Identify outliers
cat("Observation", which(cook>0.004830918), "has a cook's distance that is greater than 0.004830918")


n = nrow(dataTrain)
outliers <- which(cook > 4/n)
length(outliers)

```


**A  lot of observations (about 57) are showing up as outliers based on 4/n threshold which is 0.004830918.** 

**I will not remove the identified outliers because they might not be true outliers as it is likely for the model to show obs as outliers based on 4/n rule especially when the sample is large. Outliers usually occur in a very low frequency 1-4. They appear to be part of a heavy tail of this model rather than true outliers. ** 


```{r}
# Code for overdispersion calculation
# Calculate overdispersion parameter
model1$deviance/model1$df.res
```

**The dispersion does not seem to be a problem for this model as it is close to 1 and is not greater than 2.**


## Variable Selection 

A complete/exhaustive search to find the logistic submodel with the smallest AIC

```{r}
# Code to conduct a complete search, fit model2 and display summary

bestAIC <- bestglm(dataTrain, IC="AIC", family = binomial)
bestAIC$BestModel
```

```{r}

#model2 only using the variables from the bestAIC model

model2 <- glm(Outcome~Glucose + SkinThickness + BMI + DiabetesPedigree + Age,data=dataTrain, family="binomial")
summary(model2)

```

**Variables selected by logistic submodel model2 are: Glucose, SkinThickness, BMI, DiabetesPedigree and Age** 


```{r}
# Code to plot the AIC for the best model of each size
sizes <- cbind(0,1,2,3,4,5,6,7,8)
plot(sizes, bestAIC$Subsets$AIC, xlab="Size", ylab="Best AIC",
main="Best AICs Across Model Sizes")

```

**This plot displays the AIC for the best model of each size (ranging from 0-8) produced by the best subset selection. As expected, the best model of
size 0 (null model) has by far the largest AIC. AIC decreases as the number of variables increases; however,
from the 4-variable model on, there is little improvement in AIC. The best models of sizes 4-8 have similar
AICs, being the 5-variable model (best model), the model with the minimum AIC.**


```{r}
# Code to conduct forward stepwise, fit model3 and display summary
# Create the minimum model with only an intercept and use model1 as full model

set.seed(100)
n = nrow(dataTrain)
minmod <- glm(Outcome~1,data = dataTrain,family=binomial)

#Step forward from the minimum model using BIC as the complexity penalty
model3 = step(minmod, scope = list(lower = minmod, upper = model1), direction = "forward", k=log(n), trace=F)
summary(model3)

#names(which(summary(model3)$coefficients[,4]<.01))
```

**Variables selected are Glucose, Age, BMI, DiabetesPedigree** 


Multicollinearity test on *model1*, *model2*, and *model3*

```{r}
# Code to obtain VIF values
vif(model1)
cat("VIF Threshold Model1:", max(10, 1/(1-summary(model1)$r.squared)), "\n")

```

```{r}
# Code to obtain VIF values
vif(model2)
cat("VIF Threshold Model2:", max(10, 1/(1-summary(model2)$r.squared)), "\n")

```

```{r}
# Code to obtain VIF values
vif(model3)
cat("VIF Threshold Model3:", max(10, 1/(1-summary(model3)$r.squared)), "\n")
```
**Multicollinearity do not seem to be the issue in any of the models because the VIF threshold is 10 for all three models and none of the predictors in any of the model surpass the threshold.**


## Regularized Regression


```{r}

##ridge regression with 10-fold cross validation 

# Setting the seed 
set.seed(0)

#Optimize lambda using cross validation
model4 = cv.glmnet(as.matrix(dataTrain[,-9]), dataTrain[,9],family="binomial",alpha=0,nfolds=10,nlambda=100,type.measure="class")
cat("CV Optimized lambda:\n")

## CV Optimized lambda:
model4$lambda.min

```

**Optimal lambda: 0.05658218**


```{r}
# Setting the seed
set.seed(0)

# coefficients at optimal lambda
ridge_coef=coef(model4,s=model4$lambda.min)
cbind(ridge_coef = as.vector(ridge_coef), model1_coef = model1$coef)

```

 
**All the coefficients are slightly closer to zero than the model1 coefficients. They have shrunk due to the effect of the ridge penalty.**

**All nine variables' coefficients shrunk exactly to zero.**

**All variables are selected by the ridge regression which is expected as ridge regression does not do variable selection but only shrinks the coeffs. The coeff calculated using ridge regression at optimal lambda are much lower (~0) than the coeff generated using the logistic regression in model1 which again shows that the ridge regression shrank all of the coeffs to almost 0.**



## Prediction and Model Evaluation 

Comparing the AICs and BICs of  *model1* *model2*, and *model3*


```{r}

#Code to calculate and display AICs and BICs

set.seed(100)
n=nrow(dataTrain)
comp = rbind(model1.Full = c(AIC(model1,k=2), AIC(model1,k=log(n))),
model2.Complete.Search = c(AIC(model2,k=2), AIC(model2,k=log(n))),
model3.Forward = c(AIC(model3,k=2), AIC(model3,k=log(n))))
colnames(comp) = c("AIC","BIC")
comp
```


**Preferred model**

**model2 has the lowest AIC, and model3 has the lowest BIC. BIC tends to select smaller models than AIC, so since model3 has 4 variables selected, it has a lower BIC than model2 which has 5 variables selected.Both reduced models perform better than the full model on AIC and BIC even though we did not detect multicollinearity above while performing VIF.**


Using models to classify (0/1) on the test dataset

```{r}
# Code to calculate binary classifications. Using 0.5 as THE classification threshold. 

predProb1 <- predict(model1, dataTest, type="response")
predClass1=rep(0,nrow(dataTest))
predClass1[predProb1>.5]=1
head(predClass1)
```


```{r}
predProb2 <- predict(model2, dataTest, type="response")
predClass2=rep(0,nrow(dataTest))
predClass2[predProb2>.5]=1
head(predClass2)
```


```{r}
predProb3 <- predict(model3, dataTest, type="response")
predClass3=rep(0,nrow(dataTest))
predClass3[predProb3>.5]=1
head(predClass3)
```


```{r}
#using optimal lambda value from model 4 to perform classification

predProb4 <- as.vector(predict(model4, as.matrix(dataTest[,-9]),
s=model4$lambda.min, type="response"))
predClass4=rep(0,nrow(dataTest))
predClass4[predProb4>.5]=1
head(predClass4)
```


Calculate the accuracy, sensitivity and specificity of the predictions. 

**Note: Accuracy is the proportion of predictions that are correct overall, sensitivity is the proportion of actual positives that were correctly classified as positive, and specificity is the proportion of actual negatives which were correctly classified as negative.**


```{r}
#manual method

#pred_metrics_man = function(modelName, actualClass, predClass, totalClass) {
#cat(modelName, '\n')
#cat(c("Accuracy","Sensitivity","Specificity"), '\n')
#c(sum(predClass==actualClass)/totalClass,
#sum(predClass==actualClass&predClass==1)/nrow(dataTest[dataTest$Outcome==1,]),
#sum(predClass==actualClass&predClass==0)/nrow(dataTest[dataTest$Outcome==0,])
#)
#}

#pred_metrics_man("model1", dataTest$Outcome,
#predClass1, nrow(dataTest))

```


```{r}
# Code to calculate and display evaluation metrics
# Method 2- confusionMatrix()- Option 1
pred_metrics = function(modelName, actualClass, predClass) {
cat(modelName, '\n')
conmat <- caret::confusionMatrix(data = as.factor(predClass),
reference = as.factor(actualClass),
positive = "1")
c(conmat$overall["Accuracy"], conmat$byClass["Sensitivity"],
conmat$byClass["Specificity"])
}
pred_metrics("model1",dataTest$Outcome, predClass1)

```

```{r}
pred_metrics("model2",dataTest$Outcome, predClass2)
```

```{r}
pred_metrics("model3",dataTest$Outcome, predClass3)
```

```{r}
pred_metrics("model4",dataTest$Outcome, predClass4)
```


**Model with highest prediction accuracy:**
Model with highest prediction accuracy model4 has the best prediction accuracy (0.8260870)


Given the context of the problem, sensitivity should be prioritized. Diabetes
is a life threatening illness and testing negative for diabetes when a patient actually has diabetes could
cause a lot of dangerous and expensive health complications. Specificity would control for false positives,
which could result in a patient receiving expensive care for a condition they did not actually have, but this
treatment would almost certainly not put their life in danger and the false positive could be reversed in a
subsequent test during the course of their treatment.


## Decision Trees


```{r}
# Code to create and plot decision tree.
set.seed(300)
decision_tree <- rpart(Outcome~.,data=dataTrain, method="class")
#plotting model
rpart.plot(decision_tree, type=0,shadow.col="gray",nn=TRUE)
#box.palette= c(buzzgold,gtblue),
```


**The most significant factor in predicting diabetes according to the decision tree is glucose level.**


Advantages and disadvantages of decision tree models

With decision trees, itâ€™s visually easy to understand which factors are most
relevant to predicting whether a new patient is likely to be diagnosed with diabetes or not.
Decision trees in general also have very low bias, however they often have very large variance.



## Goodness of Fit 

We might not evaluate the goodness of fit of the logistic regression models because logistic regression does not have an error term. Additionally, we had data without replications, and replications are necessary for evaluating goodness of fit of logistic regression models. However, the data can be aggregated. As the data contains only numeric variables, they would need to be converted to categorical variables first, probably by grouping
into bands (eg Age 18-24, Age 25-34, etc.).

One can perform goodness of fit using deviance residuals or even pearson residual analysis or even perform hypothesis testing using wald/z-test. 


